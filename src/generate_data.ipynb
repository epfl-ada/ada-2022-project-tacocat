{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.freebase import *\n",
    "from utils.data_initial import *\n",
    "from utils.data_generated import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethnicity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_cmu_character_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "etnicities = list(df.actor_ethnicity.dropna().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weird result for /m/019kn7\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q161652'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Japanese people'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q31340083'}, 'sLabel': {'type': 'literal', 'value': 'Q31340083'}}]\n",
      "\n",
      "weird result for /m/0j6x8\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q170355'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Indigenous Australians'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q12060728'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Aboriginal Australians'}}]\n",
      "\n",
      "weird result for /m/062_25\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q1065371'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Italian Brazilians'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q54864438'}, 'sLabel': {'type': 'literal', 'value': 'Q54864438'}}]\n",
      "\n",
      "weird result for /m/0640_7q\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q2672654'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'History of the Jews in Morocco'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q16152214'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Moroccan Jews'}}]\n",
      "\n",
      "weird result for /m/0180zw\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q203178'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Kikuyu'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q25467191'}, 'sLabel': {'type': 'literal', 'value': 'Q25467191'}}]\n",
      "\n",
      "weird result for /m/09snp5\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q12644212'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Muhajir'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q19891823'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Muhajir diaspora'}}]\n",
      "\n",
      "weird result for /m/03x1x\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q68518'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Haudenosaunee Confederacy'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q56408633'}, 'sLabel': {'type': 'literal', 'value': 'Q56408633'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q97377726'}, 'sLabel': {'type': 'literal', 'value': 'Q97377726'}}]\n",
      "\n",
      "weird result for /m/06bkf\n",
      "[{'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q245507'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Quebeckers'}}, {'s': {'type': 'uri', 'value': 'http://www.wikidata.org/entity/Q7273097'}, 'sLabel': {'xml:lang': 'en', 'type': 'literal', 'value': 'Québécois'}}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# query the values associated to the ethnicity freebase ids contained in our data\n",
    "mappings = {}\n",
    "not_found = []\n",
    "for etn in etnicities:\n",
    "    val = query_freebase_id_from_wikidata(etn)\n",
    "    if val == None:\n",
    "        not_found.append(etn)\n",
    "    else:\n",
    "        mappings[etn] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually correct the mappings that were under an unexpected format\n",
    "mappings['/m/019kn7'] = 'Japanese'\n",
    "mappings['/m/0j6x8'] = 'Aboriginal Australians'\n",
    "mappings['/m/062_25'] = 'Italian Brazilians'\n",
    "mappings['/m/0640_7q'] = 'Moroccan Jews'\n",
    "mappings['/m/0180zw'] = 'Kikuyu'\n",
    "mappings['/m/09snp5'] = 'Muhajir'\n",
    "mappings['/m/03x1x'] = 'Haudenosaunee Confederacy'\n",
    "mappings['/m/06bkf'] = 'Quebeckers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe\n",
    "# set mappings for ids without values to NaN\n",
    "mappings.update({id: np.NaN for id in not_found})\n",
    "\n",
    "df = pd.DataFrame(data=mappings.items(), index=range(0,len(mappings)), columns=['freebase_id', 'ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "df.to_pickle(PATH_DATA_GEN + FILENAME_ETHNICITIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combine_cmu_imdb_movie'></a>\n",
    "# Combine CMU with IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb = load_imdb_title_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movie_md = load_cmu_movie_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the timespan from 1996 to 2011\n",
    "# see milestone_2.ipynb for the reasoning\n",
    "df_movie_md = df_movie_md[df_movie_md.release_date.apply(lambda d: d.year) >= 1996]\n",
    "df_movie_md = df_movie_md[df_movie_md.release_date.apply(lambda d: d.year) <= 2011]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original title to avoid double entries\n",
    "df_imdb.drop('original_title', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we merge on the title, doing a left outer join\n",
    "merged = df_movie_md.merge(df_imdb, left_on='movie_name', right_on='primary_title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only movies\n",
    "merged = merged[merged.type == 'movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since titles are not unique, keep only those with matching release date and runtime\n",
    "merged = merged[merged.release_date.apply(lambda r: r.year) == merged.start_year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/m/0h1z21s    5\n",
       "/m/09vq1kn    4\n",
       "/m/064p159    4\n",
       "/m/0j9nm_x    3\n",
       "/m/0gy0m6v    3\n",
       "             ..\n",
       "/m/04n043k    2\n",
       "/m/02pnrmw    2\n",
       "/m/0gg5jd     2\n",
       "/m/0805fwx    2\n",
       "/m/0242q0     2\n",
       "Name: movie_id_freebase, Length: 271, dtype: Int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that there are still duplicate entries\n",
    "duplicates = merged.movie_id_freebase.value_counts()\n",
    "duplicates = duplicates[duplicates > 1]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imdb_rating = load_imdb_title_rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the ones with the most imdb votes, assuming that those entries are those with the most information\n",
    "df = merged[merged.movie_id_freebase.isin(list(duplicates.index))].merge(df_imdb_rating, on='title_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = []\n",
    "for id in list(duplicates.index):\n",
    "    dft = df[df.movie_id_freebase == id]\n",
    "    ids = list(dft.sort_values('num_votes', ascending=False).iloc[1:].title_id.values)\n",
    "    to_drop += ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged[~merged.title_id.isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: movie_id_freebase, dtype: Int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that there are no duplicates left\n",
    "duplicates = merged.movie_id_freebase.value_counts()\n",
    "duplicates = duplicates[duplicates > 1]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use imdb runtime information for missing ones in the cmu dataset\n",
    "merged.runtime = merged.runtime.fillna(merged.runtime_minutes)\n",
    "merged.runtime = merged.runtime.astype(pd.Float32Dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns we don't need\n",
    "# we keep both genre columns separate, because their granularity is very different (as we saw in our eda) and this could be useful\n",
    "merged = merged.drop(['start_year', 'end_year', 'type', 'primary_title', 'runtime_minutes'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns\n",
    "merged = merged.iloc[:, [0,1,9,2,3,4,5,6,7,10,8,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "merged = merged.rename({'title_id': 'title_id_imdb', 'genres_x': 'genres_cmu', 'genres_y':'genres_imdb'}, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "merged.to_pickle(PATH_DATA_GEN + FILENAME_MOVIE_METADATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the other datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in connections between people, we do not need everything in our datasets. Hence we only consider the parts of the datasets which are relevant to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_metadata = load_movie_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = movie_metadata.title_id_imdb.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb movie crew data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_data = load_imdb_title_crew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only information for movies which are in our processed dataset\n",
    "crew_data = crew_data[crew_data.title_id.isin(movie_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_data = crew_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "crew_data.to_pickle(PATH_DATA_GEN + FILENAME_MOVIE_CREW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb movie principals data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beware that this dataframe is large, with 53001047 rows\n",
    "principals_data = load_imdb_title_principals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only information for movies which are in our processed dataset\n",
    "principals_data = principals_data[principals_data.title_id.isin(movie_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "principals_data = principals_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "principals_data.to_pickle(PATH_DATA_GEN + FILENAME_MOVIE_PRINCIPALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb movie ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data = load_imdb_title_rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only information for movies which are in our processed dataset\n",
    "ratings_data = ratings_data[ratings_data.title_id.isin(movie_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_data = ratings_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "ratings_data.to_pickle(PATH_DATA_GEN + FILENAME_MOVIE_RATINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='combine_cmu_imdb_people'></a>\n",
    "### People data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_data = load_cmu_character_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_data = load_imdb_name_basics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_ids(ids, acc):\n",
    "    for id in ids:\n",
    "        acc.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_ids = []\n",
    "\n",
    "for ids in crew_data.directors.dropna():\n",
    "    aggregate_ids(ids, people_ids)\n",
    "\n",
    "for ids in crew_data.writers.dropna():\n",
    "    aggregate_ids(ids, people_ids)\n",
    "\n",
    "for id in principals_data.persone_name_id:\n",
    "    people_ids.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only information for people which are in our processed datasets\n",
    "people_data = people_data[people_data.person_name_id.isin(people_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only care about the actor, not the character that is portrayed\n",
    "actor_data = char_data[['actor_gender', 'actor_height', 'actor_ethnicity', 'actor_name']]\n",
    "actor_data = actor_data.groupby('actor_name').agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data from imdb and from cmu\n",
    "# we combine on name, which is not ideal because of potential typos, conventions etc, but is the best we have\n",
    "merged = people_data.merge(actor_data, left_on='person_name', right_on='actor_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_max(attr):\n",
    "    if type(attr) == str or type(attr) == float:\n",
    "        return attr\n",
    "\n",
    "    # not very elegant nor efficient to create a dataframe each time\n",
    "    # but NaN values are a pain and this way at least it works right\n",
    "    df = pd.DataFrame(attr)\n",
    "    df = df.dropna().value_counts().sort_values()\n",
    "    if len(df) == 0:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return df.index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep values that occur most often\n",
    "merged.actor_gender = merged.actor_gender.map(keep_max, na_action='ignore')\n",
    "merged.actor_height = merged.actor_height.map(keep_max, na_action='ignore').astype(pd.StringDtype())\n",
    "merged.actor_ethnicity = merged.actor_ethnicity.map(keep_max, na_action='ignore').astype(pd.StringDtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.rename(columns={'actor_gender': 'gender', 'actor_height': 'heigth', 'actor_ethnicity': 'ethnicity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = merged.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "merged.to_pickle(PATH_DATA_GEN + FILENAME_PEOPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate movies per person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = load_people()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "principals = load_movie_principals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = load_movie_crew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = load_movie_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every profession in our dataset\n",
    "professions = pd.DataFrame([profession for sublist in people.primary_profession.dropna() for profession in sublist], columns=['profession']).profession.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a datafram where we aggregate the roles and movies of each person in our data\n",
    "is_in_movies = pd.DataFrame(index=people.person_name_id.values, columns=list(professions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_cell(df, x, y, value):\n",
    "    '''\n",
    "    Appends value to the cell df[x,y].\n",
    "    '''\n",
    "    if not type(df.at[x,y]) == list and np.isnan(df.at[x,y]):\n",
    "        df.at[x,y] = [value]\n",
    "    else:\n",
    "        df.at[x,y].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of how many values cannot be added due to key errors\n",
    "i = 0\n",
    "\n",
    "# add people from movie.principals\n",
    "for title_id, group in principals.groupby('title_id'):\n",
    "    for _, row in group.iterrows():\n",
    "        try:\n",
    "            append_to_cell(is_in_movies, row.persone_name_id, row.category, title_id)\n",
    "        except:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add people from movie.crew\n",
    "for _, row in crew.iterrows():\n",
    "    if type(row.directors) == list:\n",
    "        for director in row.directors:\n",
    "            try:\n",
    "                append_to_cell(is_in_movies, director, 'director', row.title_id)\n",
    "            except:\n",
    "                i += 1\n",
    "    if type(row.writers) == list:\n",
    "        for writer in row.writers:\n",
    "            try:\n",
    "                append_to_cell(is_in_movies, writer, 'writer', row.title_id)\n",
    "            except:\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We could not put 4103 entries into our dataframe.\n"
     ]
    }
   ],
   "source": [
    "print(\"We could not put {:d} entries into our dataframe.\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "is_in_movies.to_pickle(PATH_DATA_GEN + FILENAME_IS_IN_MOVIES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c6cef80ec7ac07085289c0e535b0128511f0c650ea74f2505e21f6e10b0a204"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
